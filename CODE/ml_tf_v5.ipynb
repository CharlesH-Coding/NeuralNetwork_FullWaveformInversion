{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value= 100\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "# for later versions:\n",
    "# tf.compat.v1.set_random_seed(seed_value)\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "# session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "# K.set_session(sess)\n",
    "# for later versions:\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold #--needed for cross validation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "#import keras\n",
    "import scipy as sc\n",
    "from scipy import signal\n",
    "import autokeras as ak\n",
    "#import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from autokeras import StructuredDataRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoData:\n",
    "    def __init__(self,file):\n",
    "        self.Scaler = StandardScaler()\n",
    "        self.df = pd.read_hdf(file)\n",
    "        #self.df = pd.read_pickle(\"./SuperStack.pkl\")\n",
    "        print(self.df.columns) \n",
    "        X_df= self.df[[\"SRC_Lat\",\"SRC_Lon\",\"REC_Lat\",\"REC_Lon\"]].copy()\n",
    "        y_df= self.df[[\"Greens\"]].copy()\n",
    "       \n",
    "        \n",
    "        self.X=  X_df.to_numpy()\n",
    "        self.y= y_df.to_numpy()\n",
    "        self.y=  self.y[:,0]\n",
    "        self.y = [ x.tolist() for x in self.y]\n",
    "        self.y= np.array(self.y)\n",
    "        self.y= self.y/ np.max(self.y,axis=1).reshape(-1,1)\n",
    "                         \n",
    "        \n",
    "        #self.y.shape = (380)\n",
    "        print(self.y.shape)\n",
    "        #print(self.y)\n",
    "        print(self.X.shape)\n",
    "        print(self.X[0])\n",
    "        print(self.y[0])\n",
    "testFile_h=\"./Data/Supersets/hdf/SuperStack.hdf\"  \n",
    "#testFile_s=\"SuperStack.hdf\"\n",
    "print(\"Reading in HourStacked Data...\")\n",
    "Geo = GeoData(testFile_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('-Sameness Check-')\n",
    "print(Geo.y[0,:] - Geo.y[100,:])\n",
    "print(Geo.X[0,:] - Geo.X[100,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFolds(Geo):\n",
    "    kf = KFold(n_splits=2)\n",
    "    X= Geo.X.copy()\n",
    "    y= Geo.y.copy()\n",
    "    KFold(n_splits=2, random_state=42, shuffle=True)\n",
    "    count=0\n",
    "    print(\"Will use for Cross Validation for training\")\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        if(count==0):\n",
    "            #print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "            train_x_crossV1, test_x_crossV1 = X[train_index], X[test_index]\n",
    "            train_y_crossV1, test_y_crossV1 = y[train_index], y[test_index]\n",
    "            #print(train_x_crossV1)\n",
    "            count+=1\n",
    "        else:\n",
    "            #print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "            train_x_crossV2, test_x_crossV2 = X[train_index], X[test_index]\n",
    "            train_y_crossV2, test_y_crossV2 = y[train_index], y[test_index]\n",
    "    return test_x_crossV1,test_y_crossV1,test_x_crossV2,test_y_crossV2\n",
    "\n",
    "\n",
    "X_train_h,y_train_h,X_test_h,y_test_h= kFolds(Geo) #for hoursStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the search\n",
    "search = StructuredDataRegressor(max_trials=15, loss='mean_squared_error')\n",
    "\n",
    "# perform the search/fitting to data\n",
    "search.fit(x=X_train_h, y=y_train_h, verbose=0,validation_split=0.2)\n",
    "search.fit(x=X_test_h, y=y_test_h, verbose=0,validation_split=0.2)\n",
    "\n",
    "# evaluate the model\n",
    "mae_h, _ = search.evaluate(X_test_h, y_test_h, verbose=0)\n",
    "print('Mean Square Error Average: %.3f' % mae_h)\n",
    "\n",
    "# get the best performing model\n",
    "model_h = search.export_model()\n",
    "# model_h.save('HourStackModel.h5',save_format='tf')\n",
    "# summarize the loaded model\n",
    "model_h.summary()\n",
    "# save the best performing model to file\n",
    "\n",
    "# model_h.compile()\n",
    "\n",
    "model.save('model_insurance.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best performing model\n",
    "model = search.export_model()\n",
    "# model_h.save('HourStackModel.h5',save_format='tf')\n",
    "# summarize the loaded model\n",
    "model.summary()\n",
    "# save the best performing model to file\n",
    "\n",
    "# model_h.compile()\n",
    "\n",
    "# model.save('model_insurance.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h.predict(Geo.X[50:50+1])\n",
    "\n",
    "model_h.predict(Geo.X[10:10+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_h.predict(Geo.X[10:10+1]).flatten() - model_h.predict(Geo.X[50:50+1]).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07bb33390e3b68cded9e44bbb4229561d373dc5983f7ccf9cc5dc7fb3732c92c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ml_tf_b': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
