{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 100\n",
    "\n",
    "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "# for later versions:\n",
    "# tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "# session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "# K.set_session(sess)\n",
    "# for later versions:\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold #--needed for cross validation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import signal\n",
    "# from keras.callbacks import History\n",
    "# import tensorboard\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Dense, Flatten, Dropout\n",
    "from keras.activations import relu\n",
    "from keras.layers.pooling import MaxPooling1D,GlobalMaxPool1D\n",
    "from tensorflow.keras.optimizers import  Adam\n",
    "from keras.initializers import TruncatedNormal,glorot_normal\n",
    "from autokeras import StructuredDataRegressor\n",
    "import keras_tuner as kt\n",
    "import scipy as sc\n",
    "from scipy import signal\n",
    "import pickle\n",
    "pickle.HIGHEST_PROTOCOL = 4\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd /Users/charlesh/Documents/Codes/ObsPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd ./Data/24hours/pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# for subdir, dirs, files in os.walk('./'):\n",
    "#     for file in files:\n",
    "#       df = pd.read_pickle(file)\n",
    "#       df.to_hdf('/Users/charlesh/Documents/Codes/ObsPy/Data/24hours/hdf/' + file.replace('.pkl','.hdf'),'df')\n",
    "#       print(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Might consider scikit-learnâ€™s Pipeline framework to perform the standardization during the model evaluation process, within each fold of the cross validation. This ensures that there is no data leakage from each testset cross validation fold into the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "\n",
    "# ModelSetName = 'SuperStack'\n",
    "ModelSetName = 'HourStack'\n",
    "\n",
    "class GeoData:\n",
    "    def __init__(self,file):\n",
    "        self.Scaler = StandardScaler()\n",
    "        self.df = pd.read_hdf(file)\n",
    "        #self.df = pd.read_pickle(\"./SuperStack.pkl\")\n",
    "        print(self.df.columns)\n",
    "        X_df= self.df[[\"SRC_Lat\",\"SRC_Lon\",\"REC_Lat\",\"REC_Lon\"]].copy()\n",
    "        y_df= self.df[[\"Greens\"]].copy()\n",
    "\n",
    "        self.X=  X_df.to_numpy()\n",
    "        self.y= y_df.to_numpy()\n",
    "        self.y=  self.y[:,0]\n",
    "        self.y = [ x.tolist() for x in self.y]\n",
    "        self.y= np.array(self.y)\n",
    "        self.y= self.y/ np.max(self.y,axis=1).reshape(-1,1)\n",
    "\n",
    "        #self.y.shape = (380)\n",
    "        print(self.y.shape)\n",
    "        #print(self.y)\n",
    "        print(self.X.shape)\n",
    "        print(self.X[0])\n",
    "        print(self.y[0])\n",
    "testFile= './Data/Supersets/hdf/' + ModelSetName + '.hdf'\n",
    "Geo = GeoData(testFile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2)\n",
    "X= Geo.X.copy()\n",
    "y= Geo.y.copy()\n",
    "KFold(n_splits=2, random_state=42, shuffle=True)\n",
    "count=0\n",
    "print(\"Will use for Cross Validation after training\")\n",
    "for train_index, test_index in kf.split(X):\n",
    "    if(count==0):\n",
    "        #print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "        train_x_crossV1, test_x_crossV1 = X[train_index], X[test_index]\n",
    "        train_y_crossV1, test_y_crossV1 = y[train_index], y[test_index]\n",
    "        #print(train_x_crossV1)\n",
    "        count+=1\n",
    "    else:\n",
    "        #print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "        train_x_crossV2, test_x_crossV2 = X[train_index], X[test_index]\n",
    "        train_y_crossV2, test_y_crossV2 = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = 1\n",
    "high = 50\n",
    "fs = 50\n",
    "\n",
    "def quickFilter(lowStop,highStop, fs, sig):\n",
    "    sig = signal.detrend(sig)\n",
    "    sig = sig - np.mean(sig)\n",
    "    nyq = 0.5*fs\n",
    "    wn = [lowStop/nyq,highStop/nyq]\n",
    "    order = 8\n",
    "    sos = signal.butter(order,wn,btype=\"bandpass\",fs=fs, output= \"sos\")\n",
    "    filteredOutput= signal.detrend(signal.sosfiltfilt(sos,sig))\n",
    "    filteredOutput = filteredOutput - np.mean(filteredOutput)\n",
    "    return filteredOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "def SSR(y_predict, y_truth):\n",
    "    assert y_predict.shape[0]==  y_truth.shape[0], \"In SSR: give invalid length compared to the length of true outputs\"\n",
    "    result=0\n",
    "    yDiff = [(y_truth[i] - y_predict[i])**2 for i in range(len(y_truth))]\n",
    "    result = np.sum(yDiff)\n",
    "    return result\n",
    "#SSR(outPut,yTruth)\n",
    "def singleCompare(model_singleCompare,indexNum,X,y, filterVal= False,plt_on=True): #assumed model is trained\n",
    "    output = model_singleCompare.predict(X[indexNum:indexNum+1]).flatten()\n",
    "    yTruth = y[indexNum:indexNum+1].flatten()\n",
    "    # yTruth = yTruth/np.max(yTruth)\n",
    "    # output = output/np.max(output) #---------------------------------norming here\n",
    "    if filterVal:\n",
    "        output= quickFilter(low,high,fs,output)\n",
    "        yTruth = quickFilter(low,high,fs,yTruth)\n",
    "    # yTruth = yTruth/np.max(yTruth)\n",
    "    # output = output/np.max(output) #---------------------------------norming here\n",
    "    # op_residual_mn = np.mean([yTruth,output],axis=0)\n",
    "    # output = op_residual_mn\n",
    "    ssr= SSR(output,yTruth)\n",
    "    mse= MSE(yTruth,output)\n",
    "    if plt_on:\n",
    "        t= np.arange(0,3001)\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(9,10))\n",
    "        ax1.set_title(\"Predicted\")\n",
    "        ax2.set_title(\"yTrue\")\n",
    "        ax1.plot(t*1/50, output, alpha=.8,color=\"blue\")\n",
    "        ax2.plot(t*1/50, yTruth,alpha=.8,color=\"red\",linestyle='dashed')\n",
    "        plt.show()\n",
    "        print(\"SSR for Single Sample: \", ssr)\n",
    "        print(\"MSE for Single Sample: \", mse)\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter testing\n",
    "\n",
    "#### Initial Pass on Paramater Test for HiddenLayers, outputLayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = test_x_crossV1\n",
    "y_train = test_y_crossV1\n",
    "X_test = test_x_crossV2\n",
    "y_test = test_y_crossV2\n",
    "# define the search\n",
    "OVERWRITE = False\n",
    "# search = StructuredDataRegressor(max_trials=15, loss='mean_squared_error',project_name=ModelSetName,directory='/Users/charlesh/Documents/Codes/ObsPy/ModelSets/' + ModelSetName  + '/',overwrite=OVERWRITE)\n",
    "search = StructuredDataRegressor(max_trials=15, loss='mean_squared_error',project_name=ModelSetName,directory='/Users/charlesh/Documents/Codes/ObsPy/ModelSets/' + ModelSetName  + '/',overwrite=OVERWRITE,tuner='bayesian')\n",
    "#  directory='/Users/charlesh/Documents/Codes/ObsPy/ModelSets/' + ModelSetName  + '/'\n",
    "# perform the search\n",
    "search.fit(x=X_train, y=y_train, verbose=0,validation_split=0.2)\n",
    "search.fit(x=X_test, y=y_test, verbose=0,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "mae, _ = search.evaluate(X_test, y_test, verbose=0)\n",
    "print('MAE: %.3f' % mae)\n",
    "# get the best performing model\n",
    "model = search.export_model()\n",
    "# model.compile()\n",
    "model.fit(x=X_train, y=y_train, verbose=0,validation_split=0.2)\n",
    "model.fit(x=X_test, y=y_test, verbose=0,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(model.predict(Geo.X[10:10+1]) - model.predict(Geo.X[50:50+1])))\n",
    "assert len(np.unique(model.predict(Geo.X[10:10+1]) - model.predict(Geo.X[50:50+1])))>10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'model_insurance.tf'\n",
    "model.save(model_file,save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "# from keras_visualizer import visualizer\n",
    "# visualizer(model, format='png', view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removed other output Activation functions from testing as \"linear\" produced lowest rms\n",
    "- Test for higher range of nuerons- not expecting better results maybe worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = []\n",
    "flt = True\n",
    "Ntraces = 380\n",
    "Nchoose = 71\n",
    "for N in range(Ntraces):\n",
    "    plt_on = False\n",
    "    if N==Nchoose:\n",
    "        plt_on = True\n",
    "    # if N>0:\n",
    "    #     plt_on = False\n",
    "    if plt_on:\n",
    "        print('Index: ' + str(N))\n",
    "    mse.append(singleCompare(model,N,Geo.X,Geo.y,flt,plt_on=plt_on))\n",
    "plt.plot(np.arange(0,len(mse)),mse)\n",
    "plt.title('Mean Squred Error (MSE)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Larger nueron size did not seem to dcr. RMS value\n",
    "- testing to confirm best choices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From all the comparison:\n",
    " - concluded that on avg best results are from hidden layer activation: relu , output activation: linear, hiddenlayers: 3, NueronsPerLayer: [32,16,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things left todo:\n",
    "- Test optimizer params \n",
    "- Fit on true data separate notebook\n",
    "    - once fitted\n",
    "        - write two functions\n",
    "            - userInterface: will ask for source reciever pair\n",
    "                - call predict on that point with points around it(how will we define)\n",
    "            - graphing the prediction on plot\n",
    "            - metric graph functions \n",
    "                - value loss\n",
    "                - rms\n",
    "                - \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(mse)\n",
    "# 0.04289451408557623 - seed=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_greens(model,xq,xx):\n",
    "    xq = map_to_nearest(xq,xx) #<-----comment this out after testing\n",
    "    G = model.predict(xq)\n",
    "    if np.shape(xq)[0]==1:\n",
    "        G = G.flatten()\n",
    "    G = G - np.mean(G)\n",
    "    G = G - G[0]\n",
    "    return G\n",
    "\n",
    "def map_to_nearest(src_rec,xx):\n",
    "    s_map = []\n",
    "    r_map = []\n",
    "    xm = np.unique(xx[:,0:2],axis=0)\n",
    "    for i in range(src_rec.shape[0]):\n",
    "        s_map.append(np.argmin(np.sum((src_rec[i,0:2].reshape(-1,1).T - xm)**2,axis=1)))\n",
    "        r_map.append(np.argmin(np.sum((src_rec[i,2:5].reshape(-1,1).T - xm)**2,axis=1)))\n",
    "    return np.hstack((xm[s_map,:],xm[r_map,:]))\n",
    "\n",
    "def green_to_seis(G,stf_amp=1e-5):\n",
    "    # G = signal.detrend(G)\n",
    "    # G = G - np.mean(G)\n",
    "    stf = sc.signal.unit_impulse(G.shape[1])\n",
    "    stf = -1*quickFilter(low,high, fs, stf)\n",
    "    stf = stf*(np.linspace(1,0,len(stf))**2)\n",
    "    stf = (stf/np.max(stf))*stf_amp\n",
    "    stf = stf - stf[0]\n",
    "    stf = G*0 + stf\n",
    "    # G = quickFilter(1,70, fs, G)\n",
    "    seis = sc.signal.convolve(G,stf)\n",
    "    seis = seis[:,0:G.shape[1]]\n",
    "    seis = quickFilter(1,80, fs, seis)\n",
    "    seis = seis - np.mean(seis,axis=1).reshape(-1,1)\n",
    "    seis = seis - seis[0]\n",
    "    return G,stf,seis\n",
    "\n",
    "def gaussian_noise(seis):\n",
    "    for i in range(seis.shape[0]):\n",
    "        seis[i,:] = seis[i,:] + np.random.normal(loc=10*np.mean(seis[i,:]),scale=3*np.std(seis[i,:]))\n",
    "    return seis\n",
    "\n",
    "def plot_G_STF_Seis(G,stf,seis):\n",
    "    bgc = 0.8\n",
    "    lw = 4\n",
    "    alpha = 1\n",
    "    t = np.linspace(1,len(G),len(G))/fs\n",
    "    plt.figure(figsize=(26,9))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(t,G,linewidth=lw,c='G')\n",
    "    plt.grid(alpha=alpha,visible=True)\n",
    "    plt.title('(G) Greens Function',fontweight='bold',fontsize=18)\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor((bgc*1.0, bgc*1.0, bgc*1.0))\n",
    "    plt.xticks(fontweight='bold',fontsize=13)\n",
    "    plt.yticks(fontweight='bold',fontsize=13)\n",
    "    plt.ylabel('Response Acceleration \\n ~ dV/dt',fontweight='bold',fontsize=15)\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(t,stf,linewidth=lw,c='r')\n",
    "    plt.grid(alpha=alpha,visible=True)\n",
    "    plt.title('U(t) = G(t) * S(t) + Noise(t) \\n (S) Source Function',fontweight='bold',fontsize=18)\n",
    "    plt.xlabel('Time (t) - Seconds',fontweight='bold',fontsize=15)\n",
    "    plt.ylabel('Source Displacement \\n (Meters)',fontweight='bold',fontsize=15)\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor((bgc*1.0, bgc*1.0, bgc*1.0))\n",
    "    plt.xticks(fontweight='bold',fontsize=13)\n",
    "    plt.yticks(fontweight='bold',fontsize=13)\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title('(U) Seismic Trace Function',fontweight='bold',fontsize=18)\n",
    "    plt.ylabel('Observed Velocity \\n (Meters / Second)',fontweight='bold',fontsize=15)\n",
    "    plt.plot(t,seis,linewidth=lw,c='b')\n",
    "    plt.grid(alpha=alpha,visible=True)\n",
    "    ax = plt.gca()\n",
    "    plt.xticks(fontweight='bold',fontsize=13)\n",
    "    plt.yticks(fontweight='bold',fontsize=13)\n",
    "    ax.set_facecolor((bgc*1.0, bgc*1.0, bgc*1.0))\n",
    "    plt.show()\n",
    "\n",
    "def get_box(xx,Ndeg=360):\n",
    "    box_min = np.min(xx,axis=0)\n",
    "    box_min = np.array([np.min([box_min[0],box_min[2]]),np.min([box_min[1],box_min[3]])])\n",
    "    box_max = np.max(xx,axis=0)\n",
    "    box_max = np.array([np.max([box_max[0],box_max[2]]),np.max([box_max[1],box_max[3]])])\n",
    "    # print('- Model Bounds - ')\n",
    "    # print('Min: ' + str(box_min))\n",
    "    # print('Max: ' + str(box_max))\n",
    "    Ndeg = 360\n",
    "    westwall = np.array([np.linspace(box_min[0],box_max[0],int(Ndeg/4)),np.linspace(box_min[1],box_max[1],int(Ndeg/4))*0 + box_min[1]]).T\n",
    "    eastwall = np.array([np.linspace(box_min[0],box_max[0],int(Ndeg/4)),np.linspace(box_min[1],box_max[1],int(Ndeg/4))*0 + box_max[1]]).T\n",
    "    northwall = np.array([np.linspace(box_min[0],box_max[0],int(Ndeg/4))*0+box_max[0],np.linspace(box_min[1],box_max[1],int(Ndeg/4))]).T\n",
    "    southwall = np.array([np.linspace(box_min[0],box_max[0],int(Ndeg/4))*0+box_min[0],np.linspace(box_min[1],box_max[1],int(Ndeg/4))]).T\n",
    "    box = np.vstack((eastwall,northwall,westwall,southwall))\n",
    "    return box\n",
    "\n",
    "def get_circle(Ndeg=360):\n",
    "    map_center = [40.14671214, -104.62999107]\n",
    "    r_max = 1.0\n",
    "    r_max = 1.9894304947170927\n",
    "    Lat_circle = (np.sin(np.linspace(0,2*np.pi,Ndeg))*r_max) + map_center[0]\n",
    "    Lon_circle = (np.cos(np.linspace(0,2*np.pi,Ndeg))*r_max) + map_center[1]\n",
    "    circle = np.hstack((Lat_circle.reshape(-1,1),Lon_circle.reshape(-1,1)))\n",
    "    # outbounds = np.array([[  41.466099, -103.1427  ],[  40.894699, -105.944   ],[  41.467999, -105.722099],[  38.904499, -105.833702]])\n",
    "    # Lon_circle = np.hstack((Lon_circle,outbounds[:,1]))\n",
    "    # Lat_circle = np.hstack((Lat_circle,outbounds[:,0]))\n",
    "    # plt.scatter(Lon_circle,Lat_circle,c='b')\n",
    "    # plt.scatter(np.unique(xx[:,0:2],axis=0)[:,1].flatten(),np.unique(xx[:,0:2],axis=0)[:,0].flatten(),c='r')\n",
    "    # plt.show()\n",
    "    return circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATIONS = np.unique(Geo.X[:,0:2],axis=0)\n",
    "filename = 'FWI_Bldr'\n",
    "bldr = [40.014259,-105.270724]\n",
    "xq = np.array(bldr).reshape(-1,2)\n",
    "mag = 6.5\n",
    "\n",
    "# filename ='FWI_Arbitrary'\n",
    "# xq = np.array([[  39.5  , -104.0]])\n",
    "# mag = 4.5\n",
    "\n",
    "# ----20minute plot run...-----\n",
    "# filename ='FWI_Known_Station' + str(i)\n",
    "# xq = STATIONS\n",
    "# mag = 4.5\n",
    "\n",
    "import matplotlib.patheffects as PathEffects\n",
    "Ndeg = 360\n",
    "for i in range(xq.shape[0]):\n",
    "    x_request = xq[i,:].flatten().reshape(-1,2)\n",
    "\n",
    "    import matplotlib.image as mpimg\n",
    "    xx = Geo.X\n",
    "    # i = 500\n",
    "    # xq = np.mean(xx,axis=0)[0:2].reshape(1,2) + 0.1\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    src = x_request\n",
    "    # rec = get_box(xx,Ndeg = Ndeg)\n",
    "    rec = get_circle(Ndeg = Ndeg)\n",
    "    src = rec*0 + src\n",
    "    rec = rec\n",
    "    src_rec = np.hstack((src,rec))\n",
    "    G = pred_greens(model,src_rec,xx)\n",
    "    G,stf,seis = green_to_seis(G)\n",
    "    # G = G - np.mean(G,axis=1).reshape(-1,1)\n",
    "    N = rec.shape[0]\n",
    "    K = G.shape[1]\n",
    "    trc = seis\n",
    "    trc = sc.signal.savgol_filter(trc,7*int(2*(fs/2)+1),8,axis=0)\n",
    "    trc = gaussian_noise(trc)**2\n",
    "    img = mpimg.imread('./BoulderArea.png')\n",
    "    bounds = [-105.944, -103.1427,38.904499, 41.467999]\n",
    "    plt.imshow(img,extent=[bounds[0], bounds[1],bounds[2], bounds[3]])\n",
    "    for ii in range(N-1):\n",
    "        c_src = src[ii,:]\n",
    "        c_rec = rec[ii,:]\n",
    "        lat = np.linspace(c_src[0],c_rec[0],K)\n",
    "        lon = np.linspace(c_src[1],c_rec[1],K)\n",
    "        plt.scatter(lon,lat,c=trc[ii,:],cmap='seismic',alpha=0.004)\n",
    "    plt.xticks(fontweight='bold')\n",
    "    plt.yticks(fontweight='bold')\n",
    "    plt.title('CNN Simulated Seismic Event m' + str(mag) ,fontweight='bold',fontsize=14)\n",
    "    # plt.text(bldr[1],bldr[0],'BOULDER',fontweight='bold',color='k',fontsize=20)\n",
    "    txt = plt.text(bldr[1],bldr[0],'BOULDER', size=15, color='w')\n",
    "    txt.set_path_effects([PathEffects.withStroke(linewidth=5, foreground='k')])\n",
    "    plt.scatter(x_request[0][1],x_request[0][0],marker='*',c='r',s=1000)\n",
    "    plt.scatter(x_request[0][1],x_request[0][0],marker='*',c='w',s=500)\n",
    "    ax = plt.gca()\n",
    "    plt.xlim((bounds[0],bounds[1]))\n",
    "    plt.ylim((bounds[2],bounds[3]))\n",
    "    print(ax.get_xlim())\n",
    "    print(ax.get_ylim())\n",
    "    plt.savefig('./Figures/FWI_Models/' + filename + '_' + str(i) + '.png',dpi=300)\n",
    "    print('Station: ' + str(i) + ' :: ' + str(x_request))\n",
    "    plt.show()\n",
    "print('COMPLETE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la,lo = np.meshgrid((np.array((np.max(Geo.X[:,0],axis=0), np.min(Geo.X[:,0],axis=0)))),(np.array((np.max(Geo.X[:,1],axis=0), np.min(Geo.X[:,1],axis=0)))))\n",
    "bounds = np.hstack((la.reshape(-1,1),lo.reshape(-1,1)))\n",
    "map_center = [40.14671214, -104.62999107]\n",
    "r = np.sum((bounds - map_center)**2,axis=1)**0.5\n",
    "print('--Data Bounds--')\n",
    "print(bounds)\n",
    "print('Max radii from data center: ' + str(np.max(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create --name ml_tf_ak tensorflow\n",
    "# install scipy pandas numpy matplotlib\n",
    "# clone env to ml_tf_ak_BACKUP\n",
    "# go back to ml_tf_ak\n",
    "# install keras\n",
    "# sci kit learn\n",
    "# clone env to ml_tf_ak_BACKUP - again\n",
    "# downgrade keras to 2.4.x\n",
    "# python -m ensurepip --upgrade\n",
    "# pip3 install --upgrade tensorflow\n",
    "# pip install --upgrade kt-legacy\n",
    "# pip3 install --upgrade keras-tuner\n",
    "# conda config --add channels conda-forgeconda config --set channel_priority strict\n",
    "# pip install autokeras\n",
    "# clone env to ml_tf_ak_BACKUP - final backup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
